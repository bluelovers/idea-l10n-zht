alibaba.bucket.create.dialog.hierarchical.field=分層命名空間
alibaba.bucket.create.dialog.versioning=啟用版本控制
alibaba.region.oss-ap-northeast-1=日本(東京)
alibaba.region.oss-ap-south-1=印度(孟買)
alibaba.region.oss-ap-southeast-1=澳大利亞(悉尼) 1
alibaba.region.oss-ap-southeast-2=澳大利亞(悉尼) 2
alibaba.region.oss-ap-southeast-3=馬來西亞(吉隆坡)
alibaba.region.oss-ap-southeast-5=印度尼西亞(雅加達)
alibaba.region.oss-ap-southeast-6=菲律賓(馬尼拉)
alibaba.region.oss-cn-beijing=中國(北京)
alibaba.region.oss-cn-chengdu=中國(成都)
alibaba.region.oss-cn-guangzhou=中國(廣州)
alibaba.region.oss-cn-hangzhou=中國(杭州)
alibaba.region.oss-cn-heyuan=中國(河源)
alibaba.region.oss-cn-hongkong=中國(香港)
alibaba.region.oss-cn-huhehaote=中國(呼和浩特)
alibaba.region.oss-cn-qingdao=中國(青島)
alibaba.region.oss-cn-shanghai=中國(上海)
alibaba.region.oss-cn-shenzhen=中國(深圳)
alibaba.region.oss-cn-wulanchabu=中國(烏蘭察布)
alibaba.region.oss-cn-zhangjiakou=中國(張家口)
alibaba.region.oss-eu-central-1=德國(法蘭克福)
alibaba.region.oss-eu-west-1=英國(倫敦)
alibaba.region.oss-me-east-1=阿聯酋(迪拜)
alibaba.region.oss-us-east-1=美國(弗吉尼亞州)
alibaba.region.oss-us-west-1=美國(硅谷)
alibaba.settings.credentials.file=Alibaba 憑證檔案
alibaba.task.delete.bucket.text=正在刪除存儲桶 {0}
alibaba.task.delete.directory.text=正在刪除目錄 {0}
alibaba.task.delete.directory.text2=已刪除 {0} 個物件
alibaba.task.delete.file.text=正在刪除檔案 {0}
azure.column.name.access.tier=存取層級
azure.rename.text2.indicator.deleting={0}: 正在刪除 {1}
bucket.name.is.empty.for.path=路徑“{0}”的存儲桶名稱為空
cannot.find.linode.region=找不到 {0} 的區域
cannot.open.read.stream.null.blob=無法開啟 null blob {0} 的讀取流
client.is.not.inited=客戶端未初始化
connection.error.fs.and.user.not.found=找不到 URI {0} 和使用者 {1} 的檔案系統
connection.error.hadoop.home.is.not.defined.full=HADOOP_HOME 未定義。在 Windows 上，您應該定義 HADOOP_HOME 環境變數或 Java 屬性 hadoop.home.dir。請參閱 <a href=\"https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\">Hadoop Wiki</a>，了解更多詳細資訊。
connection.error.hadoop.no.native.drivers.full=無法在 HADOOP_HOME 中找到原生驅動程序。請參閱 <a href=\"https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\">Hadoop Wiki</a>，了解更多詳細資訊。
connection.error.root.path.must.be.non.empty=根路徑必須為非空
controller.cluster.instances.error=更新集群實例錯誤
controller.cluster.steps.error=集群步驟更新錯誤
copy.failed=從 {0} 複製到 {1} 失敗。
custom.bucket.text.empty=bucket/folder,bucket2/folder/subfolder2,…
custom.bucket.text.hint=使用“,”分隔符指定來源根列表 (bucket1/folder1/folder2,bucket2/folder)
do.region.ams3=荷蘭阿姆斯特丹
do.region.fra1=德國法蘭克福
do.region.nyc3=美國紐約市
do.region.sfo=美國舊金山
do.region.sgp1=新加坡
emr.cluster.filter=按狀態篩選
emr.cluster.filter.limit=限制
emr.cluster.info.details=顯示為 JSON
emr.cluster.terminate.cluster.message=是否要終止集群 {0}?
emr.cluster.terminate.cluster.title=集群正在終止
emr.connection.creation=EMR 建立連線
emr.connection.warning.no.clusters=已連線，找不到集群
emr.connection.warning.no.clusters.desc=已建立連線，但找不到所選區域的集群。請檢查區域是否正確。
emr.connection.warning.no.clusters.desc.window=在“{0}”區域找不到集群。請檢查區域。
emr.dialog.title.select.key.info.cancel=取消
emr.dialog.title.select.key.info.msg=連線需要建立 SSH 隧道。要設定，請選擇集群的 SSH 密鑰檔案。
emr.dialog.title.select.key.info.ok=選擇 SSH 密鑰
emr.dialog.title.select.key.info.title=需要 SSH 密鑰
emr.dialog.title.select.key.ssh.file=選擇 SSH 密鑰檔案
emr.error=AWS EMR 異常
emr.error.remove.cluster=移除集群時出錯
emr.error.start.cluster=啟動集群時出錯
emr.error.stop.cluster=停止集群錯誤
emr.filter.text=篩選器:
emr.is.not.inited=EMR 客戶端未初始化
emr.key.storage.dialog.title=EMR SSH 密鑰庫
emr.keys.settings.column.key.name=鍵名
emr.keys.settings.column.key.path=路徑
emr.keys.settings.label=SSH 密鑰:
emr.keys.settings.table.empty=未提供 SSH 密鑰
emr.label.choose.key.file.for.aws.pair=為 AWS {0} 對選擇密鑰檔案
emr.remove.linked.connections.action=移除連線
emr.remove.linked.connections.desc=是否要刪除為 EMR 建立的連線?
emr.remove.linked.connections.title=EMR 連線
emr.spark.submit=EMR Spark-submit
emr.spark.submit.editor.args=實參:
emr.spark.submit.editor.jar.loc=JAR 位置:
emr.spark.submit.editor.name=名稱:
emr.step.details=顯示步驟詳細資訊
emr.step.mapper.choose=選擇 Mapper
emr.step.reducer.choose=選擇 Reducer
emr.step.s3.input.choose=選擇 S3 輸入
emr.step.s3.output.choose=選擇 S3 輸出
emr.step.script.choose=選擇 S3 腳本位置
emr.toolwindow.title=AWS EMR
error.krb5.conf=Kerberos 授權失敗。嘗試向 krb5.conf 新增 "allow_weak_crypto = true"
error.object.summary.is.not.found=沒有 {0} 的物件摘要
file.info.access.blob.type=Blob 類型
file.info.access.content.type=內容類型
file.info.access.tier=存取層級
file.info.access.tier.modified=存取層級上次修改時間
gcs.buckets.source=存儲桶源:
gcs.connection.browse.title=選擇憑證 JSON
gcs.connection.error.bucket.validation1=存儲桶名稱必須包含 3-63 個字元。包含點的名稱最多可包含 222 個字元，但每個以點分隔的組件不得超過 63 個字元。
gcs.connection.error.bucket.validation2=名稱只能包含小寫字母、數字、破折號(-)、下劃線(_)和點(.)。
gcs.connection.error.bucket.validation3=存儲桶名稱必須以數字或字母開頭和結尾。
gcs.connection.error.bucket.validation4=存儲桶名稱不能表示為用點分隔的十進制形式的 IP 地址(例如 192.168.5.4)。
gcs.connection.error.bucket.validation5=存儲桶名稱不能以“goog”前綴開頭。
gcs.connection.error.bucket.validation6=存儲桶名稱不能包含“google”或相近的錯誤拼寫，例如“g00gle”。
gcs.connection.error.cred.file.not.selected=必須為帳號選擇憑證檔案
gcs.connection.error.file.not.exists=檔案不存在
gcs.custom.url=自訂主機:
gcs.json.location.emptyText=雲存儲 JSON 位置
gcs.multibucket.update.text=Google Cloud Storage 支援多存儲桶! 您可以在連線設定中組態它們。
gcs.multibucket.update.title=BigDataTools 的新 GCS 功能
gcs.progress.details.deleting=正在刪除 {0}
gcs.project.id=專案 ID:
gcs.project.id.emptyText=可選的覆蓋專案 ID
gcs.project.id.hint=顯示特殊專案 ID 的存儲桶
gcs.public.hint=為公共存儲桶留空
gcs.sdk.install=找不到 Google Cloud SDK。<a>安裝</a>
gcs.sdk.update=Google Cloud SDK 已過時。<a>更新</a>
group.name.alibaba=Alibaba OSS
group.name.azure=Azure
group.name.dospaces=DigitalOcean Spaces
group.name.emr=AWS EMR
group.name.gcs=Google Cloud Storage
group.name.hdfs.java=HDFS
group.name.linode=Linode
group.name.minio=MinIO
group.name.s3=AWS S3
group.name.yandex=Yandex 物件存儲
group.names.hdfs.data=HDFS 問題
hdfs.column.name.access.time=存取時間
hdfs.column.name.block.size=塊大小
hdfs.column.name.group=組
hdfs.column.name.is.encrypted=已加密
hdfs.column.name.is.isErasureCoded=已使用糾刪碼
hdfs.column.name.is.isSnapshotEnabled=快照
hdfs.column.name.owner=所有者
hdfs.column.name.permission=權限
hdfs.column.name.replications=副本
hdfs.config.path.does.not.exist=指定的目錄不存在
hdfs.config.path.no.xmls.found=指定的目錄不包含任何 XML 檔案
hdfs.config.path.not.empty=組態路徑不應為空
hdfs.config.path.should.be.directory=組態路徑應該指向一個目錄
hdfs.config.path.title=Java API 組態路徑
hdfs.field.root.path=根路徑
hdfs.file.info.label.accessTime=存取時間:
hdfs.file.info.label.block.size=塊大小:
hdfs.file.info.label.group=組:
hdfs.file.info.label.isEncrypted=已加密:
hdfs.file.info.label.isErasureCoded=已使用糾刪碼:
hdfs.file.info.label.isSnapshotEnabled=已啟用快照:
hdfs.file.info.label.modificationTime=修改時間:
hdfs.file.info.label.owner=所有者:
hdfs.file.info.label.permission=權限:
hdfs.file.info.label.replication=副本:
hdfs.file.info.label.size=大小:
hdfs.is.not.inited=Hdfs 連線未初始化
hdfs.java.config.source=組態源:
hdfs.java.driver.home.path=驅動程序主路徑:
hdfs.no.xmls.in.directory=組態根中沒有 XML 檔案
hdfs.property.source.directory=組態目錄
hdfs.property.source.explicit=自訂
hdfs.root.folder.does.not.exist=根目錄 {0} 不存在
hdfs.ssh.tunnel.ssh.operation.not.supported=不支持通過 SSH 隧道進行動作。
inspection.java.custom.hdfs.format.display.name=醒目提示自定義 HDFS 檔案格式
inspection.java.invalid.file.path.display.name=醒目提示無效的 HDFS 檔案路徑
inspection.kotlin.custom.hdfs.format.display.name=醒目提示自定義 HDFS 檔案格式
inspection.kt.invalid.file.path.display.name=醒目提示無效的 HDFS 檔案路徑
inspection.non.serializable.data.in.scope.display.name=醒目提示顯示 Spark 任務中的不可序列化資料
inspection.scala.custom.hdfs.format.display.name=醒目提示自定義 HDFS 檔案格式
inspection.scala.invalid.hdfs.file.path.display.name=醒目提示無效的 HDFS 檔案路徑
invalid.format.inspection.description=醒目提示自定義 hdfs 格式。預設情況下，預期格式為 parquet、orc、sequence、json、csv 或文本。
invalid.format.inspection.template=意外的自訂檔案格式
java.wrong.path.inspection.description=在 Java 程式碼中醒目提示無效的 hdfs 路徑
kerberos.type.credentials=密碼
kerberos.type.disabled=已停用
kerberos.type.keytab=鍵表
kerberos.type.subject=JAAS 組態(專家)
kotlin.wrong.path.inspection.description=在 Kotlin 程式碼中醒目提示無效的 hdfs 路徑
linode.region.ap-south=亞太南部(新加坡)
linode.region.eu-central=歐盟中部(德國法蘭克福)
linode.region.us-east=美國東部(美國紐瓦克)
linode.region.us-southeast=美國東南部(美國亞特蘭大)
metainfo.headers.empty=無自訂標頭
metainfo.headers.key=鍵
metainfo.headers.value=值
metainfo.section.custom.headers=標頭
minio.region.text.empty=使用預設值
move.failed=從 {0} 移動到 {1} 失敗。
notification.group.orc.files=ORC 檔案
oss.file.info.label.hns.status=分層命名空間:
rfs.create.bucket.message=建立存儲桶
s3.bucket.text.empty=所有存儲桶均可見
s3.bucket.text.hint=如果此欄位為空，則所有存儲桶都將可見<br>輸入存儲桶名稱並選擇篩選器類型“符合”以處理單個存儲桶<br>使用“,”分隔存儲桶(bucket1, bucket2)
s3.column.name.etag=ETag
s3.column.name.metadata=元資料
s3.column.name.storage.class=存儲類別
s3.connection.error.ssh.without.endpoint=要使用 SSH 隧道，請為驅動程序指定一個端點
s3.empty.directories.not.allowed=不允許建立空目錄
s3.multibucket.open.settings=開啟設定
s3.multibucket.update.text=S3 相容存儲支援多存儲桶。您可以在連線設定中組態它們。
s3.multibucket.update.title=BigDataTools 的新 S3 功能
s3.operations.timeout.hint=遠端伺服器上的動作超時(以秒為單位)
scala.serializable.scope.inspection.description=醒目提示在 spark 任務作用域內使用並且會導致執行時異常的不可序列化值。
scala.serializable.scope.inspection.warning=<html>Spark 作用域 {2} 中類型 {1} 的不可序列化值 {0}</html>
scala.wrong.path.inspection.description=在 Scala 程式碼中醒目提示無效的 hdfs 路徑
settings.alibaba.region=區域:
settings.azure.auth.type=身份驗證類型:
settings.azure.connection.string=連線字串:
settings.azure.container=容器:
settings.azure.endpoint=端點:
settings.azure.password=密碼:
settings.azure.sas.token=SAS 令牌:
settings.azure.user.key=密鑰:
settings.azure.username=用戶名:
settings.bucket.filter=存儲桶篩選器:
settings.bucket.filter.type=篩選器類型:
settings.buckets.custom.list=自訂根
settings.buckets.hint=<html><b>帳戶中的所有存儲桶</b> - 執行某種 <it>list buckets</it> 請求。允許篩選結果存儲桶列表。<br><br><b>自訂根</b> - 直接請求所選根，不僅允許指定存儲桶，還允許指定目錄的完整路徑。</html>
settings.buckets.user.list=帳戶中的所有存儲桶
settings.config.from.folder=解析的組態:
settings.config.path=組態路徑:
settings.custom.roots=根:
settings.generate.kerberos=Kerberos
settings.hdfs.auth.type=身份驗證:
settings.hdfs.kerberos.type=身份驗證方法:
settings.hdfs.kinit=使用 kinit 快取
settings.hdfs.url=集群 URI:
settings.hdfs.username=Hadoop 用戶名:
settings.hdfs.username.hint=登入伺服器的用戶名。如果未指定，則使用 <i>HAD00P_USER_NAME</i> 環境變數。如果未定義此變數，則使用 <i>user.name</i> 屬性。如果啟用了 Kerberos，它將覆寫這三個值。
settings.kerberos.auth=身份驗證:
settings.kerberos.auth.kerberos=Kerberos
settings.kerberos.auth.none=無
settings.minio.endpoint=端點:
settings.properties=進階組態:
settings.s3.bucket.filter.by.region=僅限所選區域中的存儲桶
settings.s3.custom.endpoint=端點:
settings.s3.custom.region=區域:
settings.s3.custom.region.hint=需要時使用
settings.s3.region=區域:
settings.s3.region.group=AWS S3
settings.s3.selection.endpoint=與 S3 相容的存儲
settings.undefined.path=<未初始化>
settings.validation.kerberos.keytab.error=必須指定鍵表和主體
settings.validation.kerberos.password.error=必須指定主體和密碼
setup.video.tutor=連線設定教程視訊
ssh.additional.info=SSH 隧道<b>僅適用於名稱節點的動作</b>: 列出檔案，獲取元資訊。<br><br>
ssh.additional.label=(僅限 NameNode 動作)
wrong.region=找不到區域“{0}”