action.BigDataTools.CreateClusterConnectionsAction.text=新建连接
action.BigDataTools.Deploy.Configure.text=创建 Spark Submit 配置…
add.new.arbitrary.cluster.submit.connection.label=添加自定义 Spark 集群…
add.new.ssh.connection.label=添加 SSH 连接…
app.by.me.value=由我
arbitrary.cluster.wizard.create.connection.title=自定义 Spark 集群
arbitrary.cluster.wizard.default.radio.button=使用默认设置
arbitrary.cluster.wizard.select.sftp.step.desc=设置到驱动程序节点的 SFTP 连接
arbitrary.cluster.wizard.select.sftp.step.title=SFTP
arbitrary.cluster.wizard.select.spark.step.desc=设置到 Spark History Server 的连接
arbitrary.cluster.wizard.select.spark.step.title=Spark
arbitrary.cluster.wizard.select.ssh.step.desc=指定 SSH 配置以使用 Spark-Submit
arbitrary.cluster.wizard.select.ssh.step.title=SSH
arbitrary.cluster.wizard.sftp.custom.radio.button=设置自定义连接
arbitrary.cluster.wizard.sftp.none.radio.button=我不需要到驱动程序节点的 SFTP 连接
arbitrary.cluster.wizard.spark.custom.radio.button=设置自定义连接
arbitrary.cluster.wizard.spark.none.radio.button=我不需要到 Spark History Server 的连接
arbitrary.cluster.wizard.spark.step.default.uri.comment.text=(隧道主机上的 localhost)
artifact.tooltip=要在集群上运行的可执行文件的路径
check.connection.availability=检查连接可用性
check.ssh.connection.ssh.is.not.defined=未指定 SSH 配置
cluster.manager.kubernetes=Kubernetes
cluster.manager.local=局部
cluster.manager.mesos=Apache Mesos
cluster.manager.nomad=Nomad
cluster.manager.standalone=独立
cluster.manager.tooltip=服务器上配置的集群管理器。
cluster.manager.yarn=Hadoop YARN
cluster.status.loading=正在加载…
cluster.status.no.target=<无目标>
cluster.status.not.selected=<Not Selected>
configuration.description=Spark Submit 配置
configuration.name=Spark Submit
configuration.name.cluster=集群
configuration.name.local=本地(已弃用)
configuration.name.python=PySpark
configuration.name.ssh=SSH (已弃用)
configuration.options.add=其他自定义
configuration.options.add.title=添加提交选项
dialog.archives.title=选择归档文件
dialog.artifactPath.title=选择应用程序
dialog.driverClassPath.title=选择驱动程序类路径
dialog.driverLibraryPath.title=选择驱动程序库路径
dialog.files.title=选择文件
dialog.input.spark.command.description=在此处粘贴 spark-submit 命令以填写 Spark Submit 表单
dialog.input.spark.command.label=Spark 命令
dialog.input.spark.command.title=Spark 输入
dialog.jars.title=选择 Jar 文件
dialog.keytabFile.title=选择键表文件
dialog.message.adding.other.configurations.not.allowed.here=这里不允许添加其他配置。
dialog.message.failed.to.create.ssh.process=无法创建 SSH 进程
dialog.message.not.found=找不到 {0}
dialog.message.spark.home.should.be.set.to.correct.folder=$SPARK_HOME 应设置为正确的文件夹。
dialog.message.specify.application=指定应用程序
dialog.message.ssh.configuration.changed=您已为集群 {0} 选择另一个 SSH 配置。
dialog.propertiesFile.title=选择属性文件
dialog.pyfiles.title=选择 Python 文件
dialog.select.artifact.button.open.artifact.settings=工件设置
dialog.select.artifact.empty=当前项目中没有工件
dialog.select.artifact.link.open.artifact.settings=设置工件
dialog.select.artifact.title=选择依赖项工件
dialog.select.class.empty=在所选应用程序中找不到任何类
dialog.select.class.title=类名
dialog.sparkHomePath.title=选择 Spark 主目录
dialog.targetDirectory.title=选择目标目录
dialog.title.select.gradle.artifact.with.task=选择 Gradle 工件和任务
dialog.title.ssh.configuration.changed=SSH 配置已更改
dialog.workDir.title=选择工作目录
edit.ssh.configuration=编辑 SSH 配置
error.ssh=指定 SSH 配置
error.ssh.config=应指定 SSH 配置
error.target.config=选择远程目标
error.target.config.unresolved=选择有效的远程目标
exportable.SparkSubmitSettings.presentable.name=Big Data Tools Spark Submit 设置
fun.search.process.text=进程 {0}
group.ServiceView.AddSparkService.text=Spark 集群
inlay.attach.debugger.ssh=通过 SSH 隧道附加调试器
label.implicit.cluster.depend.sftp=SFTP\:
label.implicit.cluster.depend.spark.connection=Spark History\:
label.select.ssh.configuration.for.cluster=为集群 {0} 选择 SSH 配置
load.command.string=加载 spark-submit 命令
notification.group.sftpsparkfileupload=Spark 的 SFTP 文件上传
open.cluster.info.description=点击以打开文档
progress.text.upload.to.host=将 {0} 上传到主机…
pyspark=PySpark
pyspark.documentation.dataframe.schema=列\: {0}
receive.artifact.task=接收工件…
remote.target.arbitrary.cluster.remark=自定义集群
remote.target.ssh.remark=SSH
replace.with.allowed.value=替换为允许的值
row.final.command=结果提交命令
row.final.command.copy=复制 spark-submit 命令
row.final.command.hint=您可以复制或加载一个 spark-submit 命令，它会填入相应的字段
services.error.create.spark.connection.canceled.fix.label=尝试重新加载
services.error.create.spark.connection.canceled.message=被用户取消
services.error.ssh.config.is.not.found=找不到 SSH 配置
services.error.top.cannot.create.spark.connection=无法创建 Spark 连接
services.spark.connection.is.not.created=无法创建 Spark 连接
settings.additional.title=高级提交选项
settings.additional.verbose=打印额外的调试输出
settings.application=应用程序\:
settings.application.arguments=运行实参\:
settings.application.class.hint=--class 的您应用程序的主类(对于 Java / Scala 应用)。
settings.application.class.name=类\:
settings.application.class.name.error.msg=请先指定应用程序
settings.application.class.name.error.title=类选择器错误
settings.application.hint=传递给主类的 main 方法的实参(如果有)。
settings.beforeShellScript=提交脚本前
settings.beforeShellScript.hint=在 Spark Submit 之前执行的脚本。例如，“source activate py36”
settings.cluster.manager=集群管理器\:
settings.cluster.manager.proxy.user=代理用户\:
settings.cluster.manager.proxy.user.hint=<html>--proxy-user 提交应用程序时要模拟的用户。<br>此实参对 --principal/--keytab 没有影响。</html>
settings.cluster.manager.queue=队列\:
settings.cluster.manager.queue.hint=--queue 要提交到的 YARN 队列(默认值\: "default")。
settings.cluster.manager.supervise=启用监督
settings.cluster.manager.supervise.hint=--supervise 如果给定，则在驱动程序失败时重启驱动程序。
settings.debug.driver.in.debug.mode=在调试模式下
settings.debug.driver.in.run.mode=在运行模式下
settings.debug.driver.java.enable=使用调试代理启动 Spark 驱动程序
settings.debug.driver.java.not.supported=集群部署模式不支持调试
settings.debug.driver.java.port=侦听端口\:
settings.debug.driver.java.port.dynamic=<动态>
settings.debug.driver.java.suspend=挂起驱动程序\:
settings.debug.driver.java.suspend.tooltip=挂起 Spark 驱动程序进程，直到附加调试器
settings.debug.driver.java.tooltip=将 '-agentlib\:jdwp' 添加到驱动程序 Java 选项
settings.debug.title=Spark 调试
settings.dependencies.files=文件\:
settings.dependencies.files.hint=--files 要放置在每个执行程序的工作目录中的逗号分隔的文件列表。可以通过 SparkFiles.get(fileName)访问执行器中这些文件的文件路径。
settings.dependencies.jars=Jar\:
settings.dependencies.jars.hint=--jars 以逗号分隔的要包含在驱动程序和执行程序类路径中的 jar 列表。
settings.dependencies.python=Py 文件\:
settings.dependencies.python.hint=--py-files 要添加到 Python 应用的 PYTHONPATH 的 .zip、.egg 或 .py 文件的逗号分隔列表。
settings.dependencies.title=依赖项
settings.deploy.mode=部署模式\:
settings.deploy.mode.hint=<html>--deploy-mode 是在本地(“客户端”)还是在集群(“集群”)内的<br>其中一台工作进程机器上启动驱动程序。</html>
settings.deploymode.client=客户端
settings.deploymode.cluster=集群
settings.driver.class.path=驱动程序类路径\:
settings.driver.class.path.hint=<html>--driver-class-path 传递给驱动程序的额外类路径条目<br>请注意，使用 --jars 添加的 jar 会自动包含在类路径中。</html>
settings.driver.cores=驱动程序核心\:
settings.driver.cores.hint=--driver-cores 驱动程序使用的核心数，仅在集群模式下。
settings.driver.java.options=驱动程序 Java 选项\:
settings.driver.java.options.hint=--driver-java-options 传递给驱动程序的额外 Java 选项。
settings.driver.library.path=驱动程序库路径\:
settings.driver.library.path.hint=--driver-library-path 要传递给驱动程序的额外库路径条目。
settings.driver.memory=驱动程序内存\:
settings.driver.memory.hint=--driver-memory 驱动程序的内存(例如 1000M、2G)。
settings.driver.title=驱动程序
settings.envParams=环境变量
settings.envParams.hint=附加环境变量
settings.executor.archives=归档\:
settings.executor.archives.hint=<html>--archives 要提取到每个执行器的工作目录中的归档列表。</html>
settings.executor.cores=执行器核心\:
settings.executor.cores.hint=<html>--executor-cores 每个执行器的核心数量<br>(默认值\: 在 YARN 模式下为 1，在独立模式下的工作进程上为所有可用核心)。</html>
settings.executor.cores.total=执行器核心总数\:
settings.executor.cores.total.hint=--total-executor-cores 所有执行器的核心总数。
settings.executor.memory=执行器内存\:
settings.executor.memory.default=1G
settings.executor.memory.hint=--executor-memory 每个执行器的内存(例如\: 1000M、2G)
settings.executor.number=执行器数量\:
settings.executor.number.hint=<html>--num-executors 要启动的执行器数量<br>如果启用动态分配，则初始执行器数量将至少为该值。</html>
settings.executor.title=执行器
settings.integration.spark.monitoring=连接\:
settings.integration.spark.monitoring.add=新增
settings.integration.title=Spark 监控集成
settings.isInteractive=交互式
settings.isInteractive.hint=以 shell 交互式模式运行执行命令
settings.kerberos.keytab=键表\:
settings.kerberos.keytab.hint=<html>--keytab 包含上面指定主体的键表的文件的完整路径<br>此键表将通过 Secure Distributed Cache 复制到运行 Application Master 的节点，<br>用于定期更新登录票证和委托令牌。</html>
settings.kerberos.principal=主体\:
settings.kerberos.principal.hint=--principal 用于登录 KDC，同时在安全 HDFS 上运行的主体。
settings.kerberos.title=Kerberos
settings.master=主\:
settings.master.hint=--master spark\://host\:port, mesos\://host\:port, yarn, k8s\://https\://host\:port，或本地(默认值\: local[*])。
settings.maven.exclude.packages=排除软件包\:
settings.maven.exclude.packages.hint=<html>--exclude-packages 在解析 --packages 中提供的依赖项时<br>为避免依赖关系冲突而要排除的 groupId\:artifactId 列表。</html>
settings.maven.packages=软件包\:
settings.maven.packages.hint=<html>--packages 要包含在驱动程序和执行器类路径中的 jar 的 Maven 坐标列表<br>将搜索本地 Maven 仓库，然后是 Maven 中央仓库和 --repositories 给出的任何其他远程仓库<br>坐标格式应为 groupId\:artifactId\:version。</html>
settings.maven.repositories=仓库\:
settings.maven.repositories.hint=--repositories 附加远程存储库列表，用于搜索 --packages 给出的 Maven 坐标。
settings.maven.title=Maven
settings.run.target.config=远程目标\:
settings.run.target.tooltip=选择一个 Spark 集群来运行应用程序
settings.shell.title=Shell 选项
settings.shellExecutor=Shell 路径
settings.shellExecutor.hint=Shell 的路径。如果启用交互式模式或在设置提交脚本之前使用
settings.spark.config=配置\:
settings.spark.config.hint=--conf Spark 配置属性。
settings.spark.home=Spark 主目录\:
settings.spark.properties.file=属性文件\:
settings.spark.properties.file.hint=<html>--properties-file 加载额外属性的文件路径。<br>如果未指定，将查找 conf/spark-defaults.conf。</html>
settings.spark.python.sdk=使用 Python 环境运行\:
settings.spark.title=Spark 配置
settings.ssh.config=SSH 配置\:
settings.ssh.error.msg=请先选择 SSH 配置
settings.ssh.error.title=文件选择器错误
settings.ssh.target.dir=目标上传目录\:
settings.ssh.target.dir.hint=<html>输入远程主机上将上传所有本地文件的目录的路径。<br>如果所选文件已存在于此目录中，它们将被覆盖。</html>
settings.ssh.title=SFTP 选项
settings.url.artifact.name=IDEA 工件
settings.url.artifact.tooltip=IDEA 工件
settings.url.custom.name=自定义
settings.url.file.name=文件
settings.url.gcs.name=GC Storage
settings.url.gcs.tooltip=垃圾回收存储
settings.url.gradle.artifact.name=Gradle 工件
settings.url.gradle.artifact.tooltip=Gradle 工件
settings.url.hdfs.name=HDFS
settings.url.maven.artifact.name=Maven 工件
settings.url.maven.artifact.tooltip=Maven 工件
settings.url.s3.name=S3
settings.url.server.mock.desc=文件路径\:
settings.url.server.name=服务器文件
settings.url.server.tooltip=服务器文件
settings.url.upload.name=上传文件
settings.url.upload.tooltip=上传本地文件
settings.url.web.name=远程
settings.workingDirectory=工作目录
setup.ssh.config=设置 SSH 配置
spark.converter.build.run.configuration.description='PySpark' 运行配置已更改。需要转换现有配置。
spark.submit.gutter.icon.tooltip=在 Spark 集群上运行
sparkhome.tooltip=Spark 目录
upload.file.title=将文件上传到远程
upload.files.error=上传文件时出现异常。{0}
upload.files.success=已成功上传 {0} 个文件
upload.files.through.sftp.to.spark.host=通过 SFTP 上传文件
upload.target.dir.is.not.found=找不到目标目录“{0}”
work.directory.tooltip=指向脚本调用位置
